{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 | Main notebook \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from files import p2vmap\n",
    "import yaml\n",
    "import torch  # NN -> our own P2V model\n",
    "import torch.nn\n",
    "import torch.nn.functional\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "baskets = pd.read_parquet('files/market-baskets.parquet')\n",
    "del baskets[\"customer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/config-w2v.yaml') as file:\n",
    "    config_w2v = yaml.full_load(file)\n",
    "    \n",
    "with open('files/config-p2vmap.yaml') as file:\n",
    "    config_p2vmap = yaml.full_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_list = p2vmap.baskets_df_to_list(x=baskets.head(1_000_000), **config_w2v[\"baskets_df_to_list\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baskets_2 = baskets.head(1_000)\n",
    "baskets_2 = baskets_2[baskets_2.groupby(\"basket\")[\"product\"].transform(\"nunique\") > 2]\n",
    "\n",
    "data_stream_p2v = p2vmap.DataStreamP2V(\n",
    "    data=baskets_2, **config_p2vmap[\"data\"][\"data_streamer\"]\n",
    ")\n",
    "\n",
    "dl_train, dl_validation = p2vmap.build_data_loader(\n",
    "    streamer=data_stream_p2v,\n",
    "    config=config_p2vmap[\"data\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wi(n_products, size):\n",
    "    hd1 = 0.025\n",
    "    wi = torch.nn.Embedding(n_products, size, sparse=True)\n",
    "    with torch.no_grad():\n",
    "        wi.weight.uniform_(-hd1, hd1)\n",
    "    return wi\n",
    "    \n",
    "def init_wo(n_products, size):\n",
    "    hd1 = 0.025\n",
    "    wo = torch.nn.Embedding(n_products, size, sparse=True)\n",
    "    with torch.no_grad():\n",
    "        wo.weight.uniform_(-hd1, hd1)\n",
    "    return wo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_batch_of_indexes(dl=dl_train):\n",
    "    for ce, co, ns in dl_train:\n",
    "        ce, co, ns\n",
    "    return ce, co, ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wi(n_products, size):\n",
    "    hd1 = 0.025\n",
    "    wi = torch.nn.Embedding(n_products, size, sparse=True)\n",
    "    with torch.no_grad():\n",
    "        wi.weight.uniform_(-hd1, hd1)\n",
    "    return wi\n",
    "\n",
    "def initialize_wo(n_products, size):\n",
    "    hd1 = 0.025\n",
    "    wo = torch.nn.Embedding(n_products, size, sparse=True)\n",
    "    with torch.no_grad():\n",
    "        wo.weight.uniform_(-hd1, hd1)\n",
    "    return wo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_from_indexes(wi, wo, ce, co, ns):\n",
    "    wi_center = wi(ce)\n",
    "    wo_positive_samples = wo(co)\n",
    "    wo_negative_samples = wo(ns)\n",
    "    return wi_center, wo_positive_samples, wo_negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(wi_center, wo_positive_samples, wo_negative_samples):\n",
    "\n",
    "    logits_positive_samples = torch.einsum(\"ij,ij->i\", (wi_center, wo_positive_samples))\n",
    "    logits_negative_samples = torch.einsum(\"ik,ijk->ij\", (wi_center, wo_negative_samples))\n",
    "\n",
    "    loss_positive_samples = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        input=logits_positive_samples,\n",
    "        target=torch.ones_like(logits_positive_samples),\n",
    "        reduction=\"sum\",\n",
    "    )\n",
    "\n",
    "    loss_negative_samples = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        input=logits_negative_samples,\n",
    "        target=torch.zeros_like(logits_negative_samples),\n",
    "        reduction=\"sum\",\n",
    "    )\n",
    "\n",
    "    n_samples = logits_positive_samples.shape[0] * (logits_negative_samples.shape[1] + 1)\n",
    "    main_loss = (loss_positive_samples + loss_negative_samples) / n_samples\n",
    "    return main_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_products = 300\n",
    "size = 20\n",
    "\n",
    "ce, co, ns = generate_random_batch_of_indexes()\n",
    "wi, wo = initialize_wi(n_products,size), initialize_wo(n_products,size)\n",
    "wi_center, wo_positive_samples, wo_negative_samples = get_vectors_from_indexes(wi,wo,ce,co,ns)\n",
    "loss = get_loss(wi_center, wo_positive_samples, wo_negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P2V(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_products, size, batch_size, n_negative_samples):\n",
    "        super().__init__()\n",
    "\n",
    "        # add trainable variables here\n",
    "        self.wi = init_wi(n_products, size)\n",
    "        self.wo = init_wo(n_products, size)   \n",
    "             \n",
    "    def forward(self, center, context, negative_samples):\n",
    "        wi_center, wo_positive_samples, wo_negative_samples = get_vectors_from_indexes(wi,wo,center,context,negative_samples)\n",
    "        return get_loss(wi_center, wo_positive_samples, wo_negative_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerP2V:\n",
    "    \n",
    "    def __init__(self, model, train, validation, path, n_batch_log=500):\n",
    "        \n",
    "        self.model = model  # put your model here (via __init__ method)\n",
    "        self.train = train\n",
    "        self.validation = validation\n",
    "        self.optimizer = torch.optim.SparseAdam(params=list(model.parameters()))\n",
    "        self.path = path\n",
    "        os.makedirs(f\"{path}/weights\")\n",
    "        self.writer_train = torch.utils.tensorboard.SummaryWriter(\n",
    "            f\"{self.path}/runs/train\"\n",
    "        )\n",
    "        self.writer_val = torch.utils.tensorboard.SummaryWriter(f\"{self.path}/runs/val\")\n",
    "        self.n_batch_log = n_batch_log\n",
    "        self.global_batch = 0\n",
    "        self.epoch = 0\n",
    "        self.batch = 0\n",
    "\n",
    "    def fit(self, n_epochs):\n",
    "\n",
    "        for _ in range(n_epochs):\n",
    "            print(f\"epoch = {self.epoch}\")\n",
    "\n",
    "            for ce, co, ns in self.train:\n",
    "                self.batch += 1\n",
    "                self.global_batch += 1\n",
    "\n",
    "                # add training steps here:\n",
    "                #  - reset gradients\n",
    "                #  - model (forward) pass\n",
    "                #  - compute gradients\n",
    "                #  - optimizer step\n",
    "\n",
    "                self.writer_train.add_scalar(\"loss\", loss_train, self.global_batch)\n",
    "\n",
    "                if self.batch % self.n_batch_log == 1:\n",
    "                    self._callback_batch()\n",
    "\n",
    "            self._callback_epoch()\n",
    "            self.epoch += 1\n",
    "\n",
    "        self.writer_train.flush()\n",
    "        self.writer_train.close()\n",
    "        self.writer_val.flush()\n",
    "        self.writer_val.close()\n",
    "\n",
    "    def _callback_batch(self):\n",
    "        # validation loss\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            list_loss_validation = []\n",
    "            for ce, co, ns in self.validation:\n",
    "                list_loss_validation.append(self.model(ce, co, ns).item())\n",
    "            loss_validation = np.mean(list_loss_validation)\n",
    "        self.writer_val.add_scalar(\"loss\", loss_validation, self.global_batch)\n",
    "        self.model.train()\n",
    "\n",
    "        # save weights\n",
    "        np.save(\n",
    "            f\"{self.path}/weights/wi_{self.epoch:02d}_{self.batch:06d}.npy\",\n",
    "            self.get_wi(),\n",
    "        )\n",
    "\n",
    "        np.save(\n",
    "            f\"{self.path}/weights/wo_{self.epoch:02d}_{self.batch:06d}.npy\",\n",
    "            self.get_wo(),\n",
    "        )\n",
    "\n",
    "    def get_wi(self):\n",
    "        return self.model.wi.weight.detach().numpy()\n",
    "\n",
    "    def get_wo(self):\n",
    "        return self.model.wo.weight.detach().numpy()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
